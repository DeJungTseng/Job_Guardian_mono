import asyncio
import sys, os
import time
import subprocess
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, PlainTextResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

sys.path.append(os.path.join(os.path.dirname(__file__), "..", "src"))

from pydantic import BaseModel
from mcp_agent.app import MCPApp
from mcp_agent.config import get_settings, MCPSettings, MCPServerSettings
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_google import GoogleAugmentedLLM
from telemetry.config import setup_telemetry
from telemetry.tracing import trace_span

# === Data Model ===
class Essay(BaseModel):
    title: str
    body: str
    conclusion: str


# === Global Settings ===
BASE_DIR = os.path.dirname(os.path.dirname(__file__))

# Load settings from YAML files
settings = get_settings()

# Programmatically set the server path to ensure it's absolute,
# resolving the issue from loading the relative path in the YAML file.
if not settings.mcp:
    settings.mcp = MCPSettings()
settings.mcp.servers["job_guardian"] = MCPServerSettings(
    command="python",
    args=[os.path.join(BASE_DIR, "mcp_server", "server.py")],
    transport="stdio",
)

# === FastAPI åˆå§‹åŒ– ===
app = FastAPI(title="Job Guardian API", version="1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # âš ï¸ ä¸Šç·šæ™‚å¯æ”¹æˆä½ çš„ render å‰ç«¯ç¶²åŸŸ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === Agent ç‹€æ…‹ ===
mcp_app = MCPApp(name="job_guardian_agent", settings=settings)
agent_state = {"ready": False, "agent": None, "llm": None, "logs": []}


# === å•Ÿå‹•äº‹ä»¶ ===
@app.on_event("startup")
async def startup_event():
    # å•Ÿå‹• telemetry server
    telemetry_server_path = os.path.join(os.path.dirname(__file__), "telemetry_server.py")
    subprocess.Popen([sys.executable, telemetry_server_path])
    agent_state["logs"].append("ğŸ“¡ Telemetry server started.")

    setup_telemetry("job_guardian_backend")
    asyncio.create_task(start_agent())  # èƒŒæ™¯å•Ÿå‹•
    agent_state["logs"].append("ğŸš€ Agent startup task scheduled.")


async def start_agent():
    async with mcp_app.run() as agent_app:
        job_guardian_agent = Agent(
            name="job_guardian",
            instruction=(
                "ä½ æ˜¯ä¸€å€‹ä½¿ç”¨å·¥å…·ä¾†å›ç­”å•é¡Œçš„æ™ºæ…§åŠ©ç†ã€‚\n"
                "å¯ç”¨çš„å·¥å…·æœ‰ï¼š\n"
                "1. esg_hr: æŸ¥è©¢å…¬å¸çš„ ESG äººåŠ›ç™¼å±•è³‡æ–™ï¼ˆè–ªè³‡ã€ç¦åˆ©ã€å¥³æ€§ä¸»ç®¡æ¯”ä¾‹ï¼‰ã€‚\n"
                "2. labor_violations: æŸ¥è©¢å‹å‹•éƒ¨é•åå‹åŸºæ³•ç´€éŒ„ã€‚\n"
                "3. ge_work_equality_violations: æŸ¥è©¢é•åæ€§åˆ¥å·¥ä½œå¹³ç­‰æ³•ç´€éŒ„ã€‚\n\n"
                "ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šä½¿ç”¨è€…çš„å•é¡Œå‘¼å«æ­£ç¢ºçš„å·¥å…·ã€‚\n"
                "ç•¶ä½ æ”¶åˆ°å·¥å…·å›å‚³çš„çµæœå¾Œï¼Œä½ å¿…é ˆå°‡è©²çµæœæ’°å¯«æˆä¸€æ®µé€šé †çš„ä¸­æ–‡æ‘˜è¦ä¾†å›ç­”ä½¿ç”¨è€…ã€‚"
            ),
            server_names=["job_guardian"],
        )

        async with job_guardian_agent:
            llm = await job_guardian_agent.attach_llm(GoogleAugmentedLLM)
            agent_state.update({
                "ready": True,
                "agent": job_guardian_agent,
                "llm": llm
            })
            agent_state["logs"].append("âœ… Job Guardian agent initialized and ready.")

            # ä¿æŒå¸¸é§
            while True:
                await asyncio.sleep(60)


# === API ===
@app.get("/")
async def root():
    return {"status": "running", "agent_ready": agent_state["ready"]}


@app.get("/logs")
async def logs():
    """é¡¯ç¤º agent ç‹€æ…‹ï¼ˆçµ¦ telemetry iframe ç”¨ï¼‰"""
    return PlainTextResponse("\n".join(agent_state["logs"][-50:]))


@trace_span("receive_prompt")
def receive_prompt(user_query: str):
    """Logs the received user query."""
    agent_state["logs"].append(f"ğŸ” Received query: {user_query}")

@trace_span("llm_tool_call_and_synthesis")
async def execute_llm_generation(llm, user_query: str) -> str:
    """Calls the LLM to generate a response using tools."""
    result = await llm.generate_str(
        message=f"æ ¹æ“šè¼¸å…¥å…§å®¹ã€Œ{user_query}ã€ï¼Œè«‹æŸ¥è©¢å°æ‡‰çš„å…¬å¸ç´€éŒ„ä¸¦å›å‚³ç¸½çµã€‚"
    )
    return result

@trace_span("format_response")
def format_response(query: str, result: str, elapsed: float):
    """Formats the final successful response."""
    msg = f"âœ… æŸ¥è©¢å®Œæˆ ({elapsed:.2f}s)ï¼š{query}"
    agent_state["logs"].append(msg)
    return {
        "query": query,
        "result": result,
        "elapsed": elapsed,
    }

@app.post("/query")
async def query(request: Request):
    """Assistant-UI å‘¼å«çš„ä¸»è¦ API"""
    if not agent_state["ready"]:
        return JSONResponse({"error": "Agent å°šæœªåˆå§‹åŒ–å®Œæˆ"}, status_code=503)

    data = await request.json()
    user_query = data.get("query", "").strip()
    if not user_query:
        return JSONResponse({"error": "è«‹è¼¸å…¥æŸ¥è©¢å…§å®¹"}, status_code=400)

    receive_prompt(user_query)

    llm = agent_state["llm"]
    start = time.time()

    try:
        # LLM åˆ¤æ–·æ‡‰ç”¨çš„ tool ä¸¦æŸ¥è©¢ + ç¸½çµ
        result = await execute_llm_generation(llm, user_query)

        elapsed = time.time() - start
        
        response = format_response(user_query, result, elapsed)
        return response

    except Exception as e:
        err_msg = f"âŒ æŸ¥è©¢å¤±æ•—: {e}"
        agent_state["logs"].append(err_msg)
        return JSONResponse({"error": str(e)}, status_code=500)


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", 8000)))
